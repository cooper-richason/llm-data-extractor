{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# processor\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from datetime import datetime\n",
    "from llm_data_extractor.models import ExtractionResult\n",
    "from llm_data_extractor.models import ExtractionResult, LLMConfig, DBConfig, Question\n",
    "import pandas as pd\n",
    "from llm_data_extractor.db import get_connection, fetch_data, insert_results\n",
    "import concurrent.futures\n",
    "from llm_data_extractor.extract import extract_data\n",
    "import logging\n",
    "import ast\n",
    "from llm_data_extractor.log_utils import setup_logger, call_with_visible_output\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "log = setup_logger(\"processor\")  # call once per process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def format_for_db(results: List[ExtractionResult], \n",
    "                  source_id: Optional[str] = None,\n",
    "                  batch_id: Optional[str] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Format extraction results for database insertion.\n",
    "    \n",
    "    Args:\n",
    "        results: List of ExtractionResult objects\n",
    "        source_id: Optional identifier for the source document/text\n",
    "        batch_id: Optional identifier for the processing batch\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries ready for database insertion\n",
    "    \"\"\"\n",
    "    \n",
    "    formatted_results = []\n",
    "    \n",
    "    for result in results:\n",
    "        record = {\n",
    "            'question_id': result.question_id,\n",
    "            'raw_answer': result.raw_answer,\n",
    "            'parsed_answer': _serialize_answer(result.parsed_answer),\n",
    "            'confidence': result.confidence,\n",
    "            'is_valid': result.is_valid,\n",
    "            'validation_error': result.validation_error,\n",
    "            'processed_at': result.timestamp.isoformat(),\n",
    "            'source_id': source_id,\n",
    "            'batch_id': batch_id\n",
    "        }\n",
    "        \n",
    "        formatted_results.append(record)\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "\n",
    "def format_for_target_tables(results: List[ExtractionResult], \n",
    "                           questions_map: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Format results grouped by target table for direct insertion into business tables.\n",
    "    \n",
    "    Args:\n",
    "        results: List of ExtractionResult objects\n",
    "        questions_map: Map of question_id to Question objects\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with table names as keys and records as values\n",
    "    \"\"\"\n",
    "    \n",
    "    table_records = {}\n",
    "    \n",
    "    for result in results:\n",
    "        if not result.is_valid:\n",
    "            continue  # Skip invalid results\n",
    "            \n",
    "        question = questions_map.get(result.question_id)\n",
    "        if not question:\n",
    "            continue\n",
    "            \n",
    "        table_name = question.target_table\n",
    "        field_name = question.target_field\n",
    "        \n",
    "        if table_name not in table_records:\n",
    "            table_records[table_name] = {}\n",
    "        \n",
    "        # Use source_id or batch_id as the record key\n",
    "        # This assumes one record per source document per table\n",
    "        record_key = 'default'  # You might want to pass this in\n",
    "        \n",
    "        if record_key not in table_records[table_name]:\n",
    "            table_records[table_name][record_key] = {}\n",
    "        \n",
    "        table_records[table_name][record_key][field_name] = result.parsed_answer\n",
    "        table_records[table_name][record_key][f'{field_name}_confidence'] = result.confidence\n",
    "        table_records[table_name][record_key][f'{field_name}_processed_at'] = result.timestamp.isoformat()\n",
    "    \n",
    "    # Convert to list format for each table\n",
    "    formatted_tables = {}\n",
    "    for table_name, records in table_records.items():\n",
    "        formatted_tables[table_name] = list(records.values())\n",
    "    \n",
    "    return formatted_tables\n",
    "\n",
    "\n",
    "def create_summary_stats(results: List[ExtractionResult]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create summary statistics for a batch of extraction results.\n",
    "    \n",
    "    Args:\n",
    "        results: List of ExtractionResult objects\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with summary statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        return {\n",
    "            'total_questions': 0,\n",
    "            'valid_answers': 0,\n",
    "            'invalid_answers': 0,\n",
    "            'success_rate': 0.0,\n",
    "            'avg_confidence': 0.0\n",
    "        }\n",
    "    \n",
    "    total_questions = len(results)\n",
    "    valid_answers = sum(1 for r in results if r.is_valid)\n",
    "    invalid_answers = total_questions - valid_answers\n",
    "    success_rate = valid_answers / total_questions\n",
    "    \n",
    "    # Calculate average confidence for valid answers\n",
    "    valid_confidences = [r.confidence for r in results if r.is_valid and r.confidence is not None]\n",
    "    avg_confidence = sum(valid_confidences) / len(valid_confidences) if valid_confidences else 0.0\n",
    "    \n",
    "    # Get validation error categories\n",
    "    error_types = {}\n",
    "    for result in results:\n",
    "        if not result.is_valid and result.validation_error:\n",
    "            error_type = result.validation_error.split(':')[0]  # Get first part of error\n",
    "            error_types[error_type] = error_types.get(error_type, 0) + 1\n",
    "    \n",
    "    return {\n",
    "        'total_questions': total_questions,\n",
    "        'valid_answers': valid_answers,\n",
    "        'invalid_answers': invalid_answers,\n",
    "        'success_rate': round(success_rate, 3),\n",
    "        'avg_confidence': round(avg_confidence, 3),\n",
    "        'error_types': error_types,\n",
    "        'processed_at': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "def _serialize_answer(answer: Any) -> Union[str,None]:\n",
    "    \"\"\"\n",
    "    Serialize parsed answer for database storage.\n",
    "    \n",
    "    Args:\n",
    "        answer: The parsed answer value\n",
    "        \n",
    "    Returns:\n",
    "        String representation suitable for database storage\n",
    "    \"\"\"\n",
    "    \n",
    "    if answer is None:      return None\n",
    "    \n",
    "    # Handle datetime objects\n",
    "    if isinstance(answer, datetime):\n",
    "        return answer.isoformat()\n",
    "    \n",
    "    # Handle lists and complex objects\n",
    "    if isinstance(answer, (list, dict)):\n",
    "        return json.dumps(answer)\n",
    "    \n",
    "    # Handle primitive types\n",
    "    return str(answer)\n",
    "\n",
    "\n",
    "def create_audit_record(batch_id: str, \n",
    "                       source_id: str,\n",
    "                       total_questions: int,\n",
    "                       processing_time_seconds: float,\n",
    "                       llm_config: Dict[str, Any],\n",
    "                       summary_stats: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create an audit record for the extraction batch.\n",
    "    \n",
    "    Args:\n",
    "        batch_id: Unique identifier for the batch\n",
    "        source_id: Identifier for the source document\n",
    "        total_questions: Number of questions processed\n",
    "        processing_time_seconds: Time taken for processing\n",
    "        llm_config: Configuration used for LLM calls\n",
    "        summary_stats: Summary statistics from create_summary_stats\n",
    "        \n",
    "    Returns:\n",
    "        Audit record dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\n",
    "        'batch_id': batch_id,\n",
    "        'source_id': source_id,\n",
    "        'total_questions': total_questions,\n",
    "        'processing_time_seconds': round(processing_time_seconds, 2),\n",
    "        'llm_model': llm_config.get('model_name'),\n",
    "        'llm_temperature': llm_config.get('temperature'),\n",
    "        'success_rate': summary_stats.get('success_rate'),\n",
    "        'avg_confidence': summary_stats.get('avg_confidence'),\n",
    "        'processed_at': datetime.now().isoformat(),\n",
    "        'config_snapshot': json.dumps(llm_config)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "REQUIRED_COLUMNS = [\n",
    "    'input_row_id', 'input_text', 'question_id', 'question_text', \n",
    "    'answer_type', 'answer_config', 'table_name', 'field_name'\n",
    "]\n",
    "\n",
    "\n",
    "def _process_batch(batch_info: Dict[str, Any], llm_config) -> List:\n",
    "    log.debug(\"_process_batch() called\")\n",
    "    log.debug(\"batch_info type: %s\", type(batch_info))\n",
    "    results, metadata = call_with_visible_output(\n",
    "        extract_data,\n",
    "        source_text=batch_info['input_text'],\n",
    "        questions=batch_info['questions'],\n",
    "        llm_config=llm_config,\n",
    "        source_id=batch_info['input_row_id'],\n",
    "        batch_id=batch_info['input_row_id'],\n",
    "    )\n",
    "    log.debug(\"Results from extract_data(): %s\", results)\n",
    "    log.debug(\"Metadata from extract_data(): %s\", metadata)\n",
    "    for r in results:\n",
    "        r.table_name = batch_info['table_name']\n",
    "        r.field_name = batch_info['field_name']\n",
    "        r.input_row_id = batch_info['input_row_id']\n",
    "    return results\n",
    "\n",
    "def process_query(query: str, llm_config: LLMConfig, db_config: DBConfig, results_table_name: str, max_workers: int = 4) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetches data from Snowflake, processes it in parallel using an LLM, and inserts results back.\n",
    "\n",
    "    Args:\n",
    "        query: SQL query to fetch the data to be processed.\n",
    "        llm_config: Configuration for the LLM.\n",
    "        db_config: Configuration for the database connection.\n",
    "        results_table_name: Name of the table to store the results.\n",
    "        max_workers: The maximum number of threads to use for parallel processing.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing summary statistics of the processing run.\n",
    "    \"\"\"\n",
    "    log.debug('Establishing Connection')\n",
    "    # 1. Fetch data\n",
    "    conn = get_connection(db_config)\n",
    "    log.debug('Getting data')\n",
    "    try:\n",
    "        df = fetch_data(query, conn)\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    # 2. Validate columns\n",
    "    if not all(col.upper() in [c.upper() for c in df.columns] for col in REQUIRED_COLUMNS):\n",
    "        missing = set(REQUIRED_COLUMNS) - set(df.columns)\n",
    "        raise ValueError(f\"Query result is missing required columns: {missing}\")\n",
    "    \n",
    "    # Normalize column names to lowercase\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    log.debug(f'Columns: {df.columns}')\n",
    "\n",
    "    # 3. Group data into batches\n",
    "\n",
    "    def parse_config(x):\n",
    "        if isinstance(x, dict):\n",
    "            return x\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                return json.loads(x)  # works if it's valid JSON\n",
    "            except json.JSONDecodeError:\n",
    "                return ast.literal_eval(x)  # fallback for Python-style strings\n",
    "        return {}\n",
    "    log.debug('Group data into batches')\n",
    "    df['answer_config'] = df['answer_config'].apply(parse_config)\n",
    "\n",
    "    # Now this \n",
    "    # s a dict already\n",
    "    log.debug(df['answer_config'].iloc[0])\n",
    "    \n",
    "    batches = []\n",
    "    for group_name, group_df in df.groupby('input_row_id'):\n",
    "        questions = [\n",
    "            Question(\n",
    "                id=str(row['question_id']),\n",
    "                text=row['question_text'],\n",
    "                answer_type=row['answer_type'],\n",
    "                answer_config=row['answer_config'] or {}   # already a dict\n",
    "            ) for _, row in group_df.iterrows()\n",
    "        ]\n",
    "        \n",
    "        batch_info = {\n",
    "            'input_row_id': group_name,                     #input_row_id value\n",
    "            'input_text': group_df['input_text'].iloc[0],\n",
    "            'table_name': group_df['table_name'].iloc[0],\n",
    "            'field_name': group_df['field_name'].iloc[0],\n",
    "            'questions': questions\n",
    "        }\n",
    "        batches.append(batch_info)\n",
    "\n",
    "    log.debug(f'Batches: {batches}')\n",
    "    # 4. Process batches in parallel\n",
    "    all_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_batch = {executor.submit(_process_batch, batch, llm_config): batch for batch in batches}\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_batch), total=len(batches), desc=\"Processing Batches\"):\n",
    "            try:\n",
    "                results = future.result()\n",
    "                log.debug(f'Result: {results}')\n",
    "                all_results.extend(results)\n",
    "            except Exception as exc:\n",
    "                batch_info = future_to_batch[future]\n",
    "                log.debug(f\"Batch for input_row_id {batch_info['input_row_id']} generated an exception: {exc}\")\n",
    "\n",
    "    # 5. Insert results into DB\n",
    "    if all_results:\n",
    "        conn = get_connection(db_config)\n",
    "        try:\n",
    "            db_records = []\n",
    "            for res in all_results:\n",
    "                record = {\n",
    "                    \"TABLE_NAME\": res.table_name,\n",
    "                    \"FIELD_NAME\": res.field_name,\n",
    "                    \"FIELD_ID\": res.input_row_id,\n",
    "                    \"QUESTION_ID\": res.question_id,\n",
    "                    \"results\": json.dumps({\n",
    "                        \"raw_answer\": res.raw_answer, \n",
    "                        \"parsed_answer\": _serialize_answer(res.parsed_answer), \n",
    "                        \"is_valid\": res.is_valid, \n",
    "                        \"confidence\": res.confidence,\n",
    "                        \"validation_error\": res.validation_error\n",
    "                    })\n",
    "                }\n",
    "                db_records.append(record)\n",
    "            \n",
    "            insert_results(db_records, conn, results_table_name)\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    # 6. Return summary\n",
    "    summary = create_summary_stats(all_results)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
