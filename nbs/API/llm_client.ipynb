{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm_client\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp llm_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Any, Optional\n",
    "import requests\n",
    "from llm_data_extractor.models import LLMConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class LLMClientError(Exception):\n",
    "    \"\"\"Custom exception for LLM client errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_llm_response(prompt: str, config: LLMConfig) -> str:\n",
    "    \"\"\"\n",
    "    Get response from LLM API with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the LLM\n",
    "        config: LLM configuration object\n",
    "        \n",
    "    Returns:\n",
    "        Raw response string from the LLM\n",
    "        \n",
    "    Raises:\n",
    "        LLMClientError: If all retry attempts fail\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(config.max_retries):\n",
    "        try:\n",
    "            if \"gpt\" in config.model_name.lower() or \"openai\" in config.model_name.lower():\n",
    "                return _call_openai_api(prompt, config)\n",
    "            elif \"claude\" in config.model_name.lower():\n",
    "                return _call_anthropic_api(prompt, config)\n",
    "            elif _is_huggingface_model(config.model_name):\n",
    "                return _call_huggingface_api(prompt, config)\n",
    "            else:\n",
    "                return _call_generic_api(prompt, config)\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt == config.max_retries - 1:\n",
    "                raise LLMClientError(f\"Failed after {config.max_retries} attempts: {str(e)}\")\n",
    "            \n",
    "            # Exponential backoff\n",
    "            time.sleep(2 ** attempt)\n",
    "    \n",
    "    raise LLMClientError(\"Unexpected error in retry loop\")\n",
    "\n",
    "\n",
    "def _call_openai_api(prompt: str, config: LLMConfig) -> str:\n",
    "    \"\"\"Call OpenAI API.\"\"\"\n",
    "    try:\n",
    "        import openai\n",
    "    except ImportError:\n",
    "        raise LLMClientError(\"openai package not installed. Run: pip install openai\")\n",
    "    \n",
    "    # Set up client\n",
    "    client_kwargs = {}\n",
    "    if config.api_key:\n",
    "        client_kwargs[\"api_key\"] = config.api_key\n",
    "    if config.base_url:\n",
    "        client_kwargs[\"base_url\"] = config.base_url\n",
    "        \n",
    "    client = openai.OpenAI(**client_kwargs)\n",
    "    \n",
    "    # Prepare request\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    request_params = {\n",
    "        \"model\": config.model_name,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": config.temperature,\n",
    "        \"timeout\": config.timeout,\n",
    "    }\n",
    "    \n",
    "    if config.max_tokens:\n",
    "        request_params[\"max_tokens\"] = config.max_tokens\n",
    "    \n",
    "    # Make request\n",
    "    response = client.chat.completions.create(**request_params)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def _call_anthropic_api(prompt: str, config: LLMConfig) -> str:\n",
    "    \"\"\"Call Anthropic API.\"\"\"\n",
    "    try:\n",
    "        import anthropic\n",
    "    except ImportError:\n",
    "        raise LLMClientError(\"anthropic package not installed. Run: pip install anthropic\")\n",
    "    \n",
    "    client_kwargs = {}\n",
    "    if config.api_key:\n",
    "        client_kwargs[\"api_key\"] = config.api_key\n",
    "    if config.base_url:\n",
    "        client_kwargs[\"base_url\"] = config.base_url\n",
    "        \n",
    "    client = anthropic.Anthropic(**client_kwargs)\n",
    "    \n",
    "    request_params = {\n",
    "        \"model\": config.model_name,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": config.temperature,\n",
    "        \"timeout\": config.timeout,\n",
    "    }\n",
    "    \n",
    "    if config.max_tokens:\n",
    "        request_params[\"max_tokens\"] = config.max_tokens\n",
    "    \n",
    "    response = client.messages.create(**request_params)\n",
    "    return response.content[0].text\n",
    "\n",
    "\n",
    "def _call_generic_api(prompt: str, config: LLMConfig) -> str:\n",
    "    \"\"\"Generic API call for other providers.\"\"\"\n",
    "    if not config.base_url:\n",
    "        raise LLMClientError(\"base_url required for generic API calls\")\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    \n",
    "    if config.api_key:\n",
    "        headers[\"Authorization\"] = f\"Bearer {config.api_key}\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": config.model_name,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": config.temperature,\n",
    "    }\n",
    "    \n",
    "    if config.max_tokens:\n",
    "        payload[\"max_tokens\"] = config.max_tokens\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{config.base_url.rstrip('/')}/chat/completions\",\n",
    "        headers=headers,\n",
    "        json=payload,\n",
    "        timeout=config.timeout\n",
    "    )\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    result = response.json()\n",
    "    return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def _is_huggingface_model(model_name: str) -> bool:\n",
    "    \"\"\"Check if model name indicates a Hugging Face model.\"\"\"\n",
    "    hf_indicators = [\n",
    "        \"meta-llama\", \"mistralai\", \"microsoft\", \"google\", \"qwen\", \n",
    "        \"codellama\", \"mixtral\", \"phi\", \"gemma\", \"llama\", \"mistral\"\n",
    "    ]\n",
    "    return any(indicator in model_name.lower() for indicator in hf_indicators)\n",
    "\n",
    "\n",
    "def _call_huggingface_api(prompt: str, config: LLMConfig) -> str:\n",
    "    \"\"\"Call Hugging Face Router API using OpenAI-compatible format.\"\"\"\n",
    "\n",
    "\n",
    "    API_URL = \"https://router.huggingface.co/v1/chat/completions\"\n",
    "        \n",
    "    if not config.api_key:\n",
    "        raise LLMClientError(\"Hugging Face API token required. Set api_key in LLMConfig.\")\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {config.api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    \n",
    "    # Prepare payload using OpenAI chat completions format\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"model\": config.model_name,\n",
    "        \"temperature\": config.temperature,\n",
    "    }\n",
    "    \n",
    "    #print('\\nPayload:',payload)\n",
    "\n",
    "    if config.max_tokens:\n",
    "        payload[\"max_tokens\"] = config.max_tokens\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            API_URL,\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=config.timeout\n",
    "        )\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        print('\\nResponse:', result)\n",
    "        \n",
    "        # Extract content from OpenAI-style response\n",
    "        if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "            choice = result[\"choices\"][0]\n",
    "            if \"message\" in choice:\n",
    "                return choice[\"message\"][\"content\"]\n",
    "            elif \"text\" in choice:\n",
    "                return choice[\"text\"]\n",
    "        \n",
    "        # Fallback for unexpected format\n",
    "        raise LLMClientError(f\"Unexpected response format: {result}\")\n",
    "            \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response.status_code == 503:\n",
    "            raise LLMClientError(\"Model is loading on Hugging Face. Please wait and retry.\")\n",
    "        elif e.response.status_code == 429:\n",
    "            raise LLMClientError(\"Rate limit exceeded. Please wait before retrying.\")\n",
    "        elif e.response.status_code == 401:\n",
    "            raise LLMClientError(\"Invalid Hugging Face token. Check your API key.\")\n",
    "        else:\n",
    "            error_text = \"\"\n",
    "            try:\n",
    "                error_detail = e.response.json()\n",
    "                error_text = error_detail.get(\"error\", {}).get(\"message\", str(error_detail))\n",
    "            except:\n",
    "                error_text = e.response.text\n",
    "            raise LLMClientError(f\"HTTP error {e.response.status_code}: {error_text}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise LLMClientError(f\"Request failed: {str(e)}\")\n",
    "\n",
    "\n",
    "def parse_llm_json_response(response: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse JSON response from LLM, handling common formatting issues.\n",
    "    \n",
    "    Args:\n",
    "        response: Raw response string from LLM\n",
    "        \n",
    "    Returns:\n",
    "        Parsed JSON object\n",
    "        \n",
    "    Raises:\n",
    "        LLMClientError: If JSON cannot be parsed\n",
    "    \"\"\"\n",
    "    # Clean up common issues\n",
    "    cleaned_response = response.strip()\n",
    "    \n",
    "    # Remove markdown code block markers if present\n",
    "    if cleaned_response.startswith(\"```json\"):\n",
    "        cleaned_response = cleaned_response[7:]\n",
    "    if cleaned_response.startswith(\"```\"):\n",
    "        cleaned_response = cleaned_response[3:]\n",
    "    if cleaned_response.endswith(\"```\"):\n",
    "        cleaned_response = cleaned_response[:-3]\n",
    "    \n",
    "    cleaned_response = cleaned_response.strip()\n",
    "    \n",
    "    try:\n",
    "        return json.loads(cleaned_response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise LLMClientError(f\"Failed to parse JSON response: {str(e)}\\nResponse: {response[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
