{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract\n",
    "\n",
    "> Extract Data from text using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import time\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from llm_data_extractor.models import Question, ExtractionResult, LLMConfig\n",
    "from llm_data_extractor.prompt_builder import build_prompt\n",
    "from llm_data_extractor.llm_client import get_llm_response, parse_llm_json_response, LLMClientError\n",
    "from llm_data_extractor.validator import validate_answer\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _process_question_response(question: Question, \n",
    "                              llm_responses: List[Dict], \n",
    "                              raw_response: str) -> ExtractionResult:\n",
    "    \"\"\"Process a single question's response from the LLM.\"\"\"\n",
    "    \n",
    "    # Find matching response\n",
    "    question_response = None\n",
    "    for response in llm_responses:\n",
    "        # Handle both dict and potentially malformed responses\n",
    "        if isinstance(response, dict) and response.get('question_id') == question.id:\n",
    "            question_response = response\n",
    "            break\n",
    "    \n",
    "    if not question_response:\n",
    "        return ExtractionResult(\n",
    "            question_id=question.id,\n",
    "            raw_answer=\"\",\n",
    "            parsed_answer=None,\n",
    "            confidence=0.0,\n",
    "            is_valid=False,\n",
    "            validation_error=\"No response found for question\"\n",
    "        )\n",
    "    \n",
    "    # Safely extract answer and confidence\n",
    "    try:\n",
    "        raw_answer = str(question_response.get('answer', ''))\n",
    "        confidence = float(question_response.get('confidence', 0.0))\n",
    "    except (AttributeError, TypeError, ValueError) as e:\n",
    "        logger.error(f\"Error extracting answer/confidence: {str(e)}\")\n",
    "        return ExtractionResult(\n",
    "            question_id=question.id,\n",
    "            raw_answer=\"\",\n",
    "            parsed_answer=None,\n",
    "            confidence=0.0,\n",
    "            is_valid=False,\n",
    "            validation_error=f\"Response format error: {str(e)}\"\n",
    "        )\n",
    "    \n",
    "    # Validate the answer\n",
    "    parsed_answer, is_valid, validation_error = validate_answer(raw_answer, question)\n",
    "    \n",
    "    return ExtractionResult(\n",
    "        question_id=question.id,\n",
    "        raw_answer=raw_answer,\n",
    "        parsed_answer=parsed_answer,\n",
    "        confidence=confidence,\n",
    "        is_valid=is_valid,\n",
    "        validation_error=validation_error if validation_error else None\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _fallback_parse_response(raw_response: str, questions: List[Question]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fallback parser for when structured JSON parsing fails.\n",
    "    Attempts to extract answers using simple text parsing.\n",
    "    \"\"\"\n",
    "    \n",
    "    responses = []\n",
    "    lines = raw_response.split('\\n')\n",
    "    \n",
    "    for question in questions:\n",
    "        # Look for patterns like \"Question 1:\", \"Q1:\", question ID, etc.\n",
    "        answer = \"\"\n",
    "        confidence = 0.0\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Check if this line references our question\n",
    "            if (question.id in line or \n",
    "                f\"Question {questions.index(question) + 1}\" in line or\n",
    "                question.text[:20] in line):\n",
    "                \n",
    "                # Look ahead for the answer\n",
    "                for j in range(i + 1, min(i + 5, len(lines))):\n",
    "                    next_line = lines[j].strip()\n",
    "                    if next_line and not next_line.startswith('Q'):\n",
    "                        answer = next_line\n",
    "                        break\n",
    "                \n",
    "                break\n",
    "        \n",
    "        responses.append({\n",
    "            'question_id': question.id,\n",
    "            'answer': answer,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def extract_data(source_text: str, \n",
    "                questions: List[Question], \n",
    "                llm_config: LLMConfig,\n",
    "                source_id: Optional[str] = None,\n",
    "                batch_id: Optional[str] = None) -> Tuple[List[ExtractionResult], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract structured data from unstructured text using LLM.\n",
    "    \n",
    "    Args:\n",
    "        source_text: The text to extract data from\n",
    "        questions: List of questions to answer\n",
    "        llm_config: LLM configuration\n",
    "        source_id: Optional identifier for the source\n",
    "        batch_id: Optional batch identifier\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (extraction_results, metadata)\n",
    "    \"\"\"\n",
    "    \n",
    "    if not questions:\n",
    "        return [], {'error': 'No questions provided'}\n",
    "    \n",
    "    # Generate batch_id if not provided\n",
    "    if not batch_id:\n",
    "        batch_id = str(uuid.uuid4())\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    print('-'*80); print('QUESTIONS'); print('-'*80); print(questions); print('-'*80);\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Build prompt\n",
    "        prompt = build_prompt(questions, source_text)\n",
    "        \n",
    "        # Step 2: Get LLM response\n",
    "        raw_response = get_llm_response(prompt, llm_config)\n",
    "        print('-'*80); print('RAW RESPONSE'); print('-'*80)\n",
    "        print(raw_response)\n",
    "        print('-'*80)\n",
    "        # Step 3: Parse JSON response\n",
    "        try:\n",
    "            parsed_response = parse_llm_json_response(raw_response)\n",
    "            llm_responses = parsed_response.get('responses', [])\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to parse structured JSON response: {str(e)}\")\n",
    "            # Fallback: try to extract answers from unstructured response\n",
    "            llm_responses = _fallback_parse_response(raw_response, questions)\n",
    "        \n",
    "        # Step 4: Validate and create results\n",
    "        results = []\n",
    "        for question in questions:\n",
    "            result = _process_question_response(question, llm_responses, raw_response)\n",
    "            results.append(result)\n",
    "        \n",
    "        # Step 5: Create metadata\n",
    "        processing_time = time.time() - start_time\n",
    "        metadata = {\n",
    "            'batch_id': batch_id,\n",
    "            'source_id': source_id,\n",
    "            'processing_time_seconds': processing_time,\n",
    "            'total_questions': len(questions),\n",
    "            'raw_llm_response': raw_response,\n",
    "            'prompt_used': prompt,\n",
    "            'llm_config': llm_config.__dict__,\n",
    "        }\n",
    "        \n",
    "        return results, metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle catastrophic failures\n",
    "        processing_time = time.time() - start_time\n",
    "        error_results = []\n",
    "        \n",
    "        for question in questions:\n",
    "            error_result = ExtractionResult(\n",
    "                question_id=question.id,\n",
    "                raw_answer=\"\",\n",
    "                parsed_answer=None,\n",
    "                confidence=0.0,\n",
    "                is_valid=False,\n",
    "                input_row_id=None,\n",
    "                validation_error=f\"Processing failed: {str(e)}\"\n",
    "            )\n",
    "            error_results.append(error_result)\n",
    "        \n",
    "        metadata = {\n",
    "            'batch_id': batch_id,\n",
    "            'source_id': source_id,\n",
    "            'processing_time_seconds': processing_time,\n",
    "            'total_questions': len(questions),\n",
    "            'error': str(e),\n",
    "            'llm_config': llm_config.__dict__\n",
    "        }\n",
    "        \n",
    "        return error_results, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
