{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import threading\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import pandas as pd\n",
    "from dataclasses import asdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from llm_data_extractor.models import Question, LLMConfig, AnswerType\n",
    "from llm_data_extractor.extract import extract_data\n",
    "from llm_data_extractor.snowflake_client import (\n",
    "    SnowflakeConfig, insert_extraction_results, format_results_for_snowflake,\n",
    "    get_pending_extractions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BatchProcessor:\n",
    "    \"\"\"Main orchestrator for batch processing LLM extractions with Snowflake using workers.\"\"\"\n",
    "    \n",
    "    def __init__(self, snowflake_config: SnowflakeConfig, llm_config: LLMConfig, max_workers: int = 4):\n",
    "        self.snowflake_config = snowflake_config\n",
    "        self.llm_config = llm_config\n",
    "        self.max_workers = max_workers\n",
    "        self._insert_lock = threading.Lock()  # For thread-safe database inserts\n",
    "        \n",
    "    def process_from_snowflake(self, \n",
    "                              query: str,\n",
    "                              query_params: Optional[Dict[str, Any]] = None,\n",
    "                              max_batches: Optional[int] = None,\n",
    "                              dry_run: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process pending extractions by running a query against Snowflake.\n",
    "        \n",
    "        Args:\n",
    "            query: SQL query to get pending extraction records\n",
    "            query_params: Optional parameters for the query\n",
    "            max_batches: Maximum number of input batches to process\n",
    "            dry_run: If True, don't insert results back to database\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with processing summary and statistics\n",
    "            \n",
    "        Expected query result columns:\n",
    "            - INPUT_ROW_ID: Identifier for the input record\n",
    "            - INPUT_TEXT: Text to extract data from  \n",
    "            - QUESTION_ID: Question identifier\n",
    "            - QUESTION_TEXT: The question text\n",
    "            - ANSWER_TYPE: Type of expected answer\n",
    "            - ANSWER_CONFIG: JSON configuration for the answer\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Get data from Snowflake using user's query\n",
    "            logger.info(\"Fetching pending extractions from Snowflake...\")\n",
    "            pending_records = get_pending_extractions(\n",
    "                snowflake_config=self.snowflake_config,\n",
    "                query=query,\n",
    "                params=query_params\n",
    "            )\n",
    "            \n",
    "            if not pending_records:\n",
    "                logger.info(\"No pending extractions found\")\n",
    "                return self._empty_result(start_time)\n",
    "            \n",
    "            # Convert to DataFrame and use existing processing logic\n",
    "            df = pd.DataFrame(pending_records)\n",
    "            \n",
    "            # Ensure column names are uppercase (Snowflake default)\n",
    "            df.columns = [col.upper() for col in df.columns]\n",
    "            \n",
    "            # Validate required columns\n",
    "            required_columns = ['INPUT_ROW_ID', 'INPUT_TEXT', 'QUESTION_ID', 'QUESTION_TEXT', 'ANSWER_TYPE', 'ANSWER_CONFIG']\n",
    "            missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "            \n",
    "            if missing_columns:\n",
    "                raise ValueError(f\"Query result missing required columns: {missing_columns}\")\n",
    "            \n",
    "            logger.info(f\"Retrieved {len(df)} pending extraction records from Snowflake\")\n",
    "            \n",
    "            # Use existing dataframe processing logic\n",
    "            return self.process_dataframe(df, max_batches, dry_run)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process from Snowflake: {str(e)}\")\n",
    "            return {\n",
    "                'status': 'failed',\n",
    "                'error': str(e),\n",
    "                'processing_time_seconds': round(time.time() - start_time, 2)\n",
    "            }\n",
    "    \n",
    "    def process_dataframe(self, \n",
    "                         df: pd.DataFrame,\n",
    "                         max_batches: Optional[int] = None,\n",
    "                         dry_run: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process pending extractions from a provided dataframe using concurrent workers.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with pending extraction records (from your SQL query)\n",
    "            max_batches: Maximum number of input batches to process\n",
    "            dry_run: If True, don't insert results back to database\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with processing summary and statistics\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if df.empty:\n",
    "                logger.info(\"No pending extractions in provided dataframe\")\n",
    "                return self._empty_result(start_time)\n",
    "            \n",
    "            logger.info(f\"Processing {len(df)} pending extraction records with {self.max_workers} workers\")\n",
    "            \n",
    "            # Step 1: Group records into batches by input_row_id\n",
    "            batches = self._batch_dataframe_records(df)\n",
    "            \n",
    "            # Limit batches if requested\n",
    "            if max_batches:\n",
    "                batch_keys = list(batches.keys())[:max_batches]\n",
    "                batches = {k: batches[k] for k in batch_keys}\n",
    "                logger.info(f\"Limited to {max_batches} batches\")\n",
    "            \n",
    "            # Step 2: Process batches concurrently\n",
    "            processing_stats = {\n",
    "                'batches_processed': 0,\n",
    "                'batches_succeeded': 0,\n",
    "                'batches_failed': 0,\n",
    "                'total_questions': 0,\n",
    "                'total_valid_answers': 0,\n",
    "                'total_invalid_answers': 0,\n",
    "                'total_inserted': 0,\n",
    "                'errors': []\n",
    "            }\n",
    "            \n",
    "            # Use ThreadPoolExecutor for concurrent processing\n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                # Submit all batch processing tasks\n",
    "                future_to_batch = {\n",
    "                    executor.submit(self._process_and_insert_batch, batch_data, dry_run): batch_key \n",
    "                    for batch_key, batch_data in batches.items()\n",
    "                }\n",
    "                \n",
    "                # Process completed tasks as they finish\n",
    "                for future in as_completed(future_to_batch):\n",
    "                    batch_key = future_to_batch[future]\n",
    "                    \n",
    "                    try:\n",
    "                        batch_result = future.result()\n",
    "                        \n",
    "                        # Update stats thread-safely\n",
    "                        with self._insert_lock:\n",
    "                            processing_stats['batches_processed'] += 1\n",
    "                            processing_stats['batches_succeeded'] += 1\n",
    "                            processing_stats['total_questions'] += batch_result['stats']['total_questions']\n",
    "                            processing_stats['total_valid_answers'] += batch_result['stats']['valid_answers']\n",
    "                            processing_stats['total_invalid_answers'] += batch_result['stats']['invalid_answers']\n",
    "                            processing_stats['total_inserted'] += batch_result['rows_inserted']\n",
    "                        \n",
    "                        logger.info(f\"✓ Batch {batch_key}: {batch_result['stats']['success_rate']:.1%} success, \"\n",
    "                                  f\"{batch_result['rows_inserted']} rows inserted\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        with self._insert_lock:\n",
    "                            processing_stats['batches_failed'] += 1\n",
    "                            processing_stats['errors'].append(f\"Batch {batch_key}: {str(e)}\")\n",
    "                        logger.error(f\"✗ Batch {batch_key} failed: {str(e)}\")\n",
    "            \n",
    "            # Calculate overall success rate\n",
    "            total_questions = processing_stats['total_valid_answers'] + processing_stats['total_invalid_answers']\n",
    "            overall_success_rate = (processing_stats['total_valid_answers'] / total_questions) if total_questions > 0 else 0\n",
    "            \n",
    "            # Final summary\n",
    "            summary = {\n",
    "                'status': 'completed' if not processing_stats['errors'] else 'completed_with_errors',\n",
    "                'total_pending_records': len(df),\n",
    "                'unique_batches': len(batches),\n",
    "                'batches_processed': processing_stats['batches_processed'],\n",
    "                'batches_succeeded': processing_stats['batches_succeeded'],\n",
    "                'batches_failed': processing_stats['batches_failed'],\n",
    "                'total_questions': processing_stats['total_questions'],\n",
    "                'total_valid_answers': processing_stats['total_valid_answers'],\n",
    "                'total_invalid_answers': processing_stats['total_invalid_answers'],\n",
    "                'overall_success_rate': round(overall_success_rate, 3),\n",
    "                'results_inserted': processing_stats['total_inserted'],\n",
    "                'processing_time_seconds': round(time.time() - start_time, 2),\n",
    "                'avg_batch_time': round((time.time() - start_time) / len(batches), 2) if batches else 0,\n",
    "                'workers_used': self.max_workers,\n",
    "                'errors': processing_stats['errors']\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"🎉 Processing completed: {summary['overall_success_rate']:.1%} success rate, \"\n",
    "                       f\"{summary['results_inserted']} records inserted, \"\n",
    "                       f\"{summary['processing_time_seconds']:.1f}s total\")\n",
    "            \n",
    "            return summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Batch processing failed: {str(e)}\")\n",
    "            return {\n",
    "                'status': 'failed',\n",
    "                'error': str(e),\n",
    "                'processing_time_seconds': round(time.time() - start_time, 2)\n",
    "            }\n",
    "    \n",
    "    def _process_and_insert_batch(self, batch_data: Dict[str, Any], dry_run: bool) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a single batch and immediately insert results.\n",
    "        This method runs in a worker thread.\n",
    "        \n",
    "        Args:\n",
    "            batch_data: Dictionary containing input_text and questions\n",
    "            dry_run: If True, don't actually insert to database\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with batch results and insertion count\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_start_time = time.time()\n",
    "        batch_id = batch_data['input_row_id']\n",
    "        \n",
    "        try:\n",
    "            # Convert question dictionaries to Question objects\n",
    "            questions = []\n",
    "            for q_data in batch_data['questions']:\n",
    "                try:\n",
    "                    question = Question(\n",
    "                        id=q_data['id'],\n",
    "                        text=q_data['text'],\n",
    "                        answer_type=AnswerType(q_data['answer_type']),\n",
    "                        target_table=q_data['target_table'],\n",
    "                        target_field=q_data['target_field'],\n",
    "                        constraints=q_data.get('constraints', {})\n",
    "                    )\n",
    "                    questions.append(question)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to create Question object for {q_data['id']}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if not questions:\n",
    "                raise Exception(\"No valid questions found in batch\")\n",
    "            \n",
    "            # Extract data using LLM\n",
    "            logger.debug(f\"Worker processing batch {batch_id} with {len(questions)} questions\")\n",
    "            extraction_results, metadata = extract_data(\n",
    "                source_text=batch_data['input_text'],\n",
    "                questions=questions,\n",
    "                llm_config=self.llm_config,\n",
    "                source_id=batch_id\n",
    "            )\n",
    "            \n",
    "            # Format results for Snowflake\n",
    "            snowflake_records = format_results_for_snowflake(\n",
    "                extraction_results=extraction_results,\n",
    "                input_row_id=batch_id,\n",
    "                metadata=metadata\n",
    "            )\n",
    "            \n",
    "            # Insert results immediately (thread-safe)\n",
    "            rows_inserted = 0\n",
    "            if not dry_run and snowflake_records:\n",
    "                with self._insert_lock:\n",
    "                    rows_inserted = insert_extraction_results(self.snowflake_config, snowflake_records)\n",
    "            elif dry_run:\n",
    "                rows_inserted = len(snowflake_records)  # For dry run reporting\n",
    "            \n",
    "            # Calculate statistics\n",
    "            valid_count = sum(1 for r in extraction_results if r.is_valid)\n",
    "            invalid_count = len(extraction_results) - valid_count\n",
    "            success_rate = valid_count / len(extraction_results) if extraction_results else 0\n",
    "            \n",
    "            batch_time = time.time() - batch_start_time\n",
    "            \n",
    "            return {\n",
    "                'batch_id': batch_id,\n",
    "                'rows_inserted': rows_inserted,\n",
    "                'processing_time': batch_time,\n",
    "                'stats': {\n",
    "                    'total_questions': len(extraction_results),\n",
    "                    'valid_answers': valid_count,\n",
    "                    'invalid_answers': invalid_count,\n",
    "                    'success_rate': success_rate,\n",
    "                    'avg_confidence': metadata.get('summary_stats', {}).get('avg_confidence', 0.0)\n",
    "                },\n",
    "                'metadata': metadata\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log the full error for this batch\n",
    "            logger.error(f\"Batch {batch_id} processing failed after {time.time() - batch_start_time:.1f}s: {str(e)}\")\n",
    "            raise e\n",
    "    \n",
    "    def _batch_dataframe_records(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Group dataframe records by input_row_id for batch processing.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with pending extraction records\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with input_row_id as key and batch info as value\n",
    "        \"\"\"\n",
    "        \n",
    "        batches = {}\n",
    "        \n",
    "        for _, record in df.iterrows():\n",
    "            input_row_id = str(record['INPUT_ROW_ID'])\n",
    "            \n",
    "            if input_row_id not in batches:\n",
    "                batches[input_row_id] = {\n",
    "                    'input_row_id': input_row_id,\n",
    "                    'input_text': record['INPUT_TEXT'],\n",
    "                    'questions': [],\n",
    "                    'question_configs': {}\n",
    "                }\n",
    "            \n",
    "            # Parse answer_config JSON if it exists\n",
    "            answer_config = {}\n",
    "            if pd.notna(record['ANSWER_CONFIG']) and record['ANSWER_CONFIG']:\n",
    "                try:\n",
    "                    if isinstance(record['ANSWER_CONFIG'], dict):\n",
    "                        answer_config = record['ANSWER_CONFIG']\n",
    "                    else:\n",
    "                        import json\n",
    "                        answer_config = json.loads(record['ANSWER_CONFIG'])\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    logger.warning(f\"Invalid ANSWER_CONFIG for question {record['QUESTION_ID']}\")\n",
    "                    answer_config = {}\n",
    "            \n",
    "            # Create question object\n",
    "            question_data = {\n",
    "                'id': str(record['QUESTION_ID']),\n",
    "                'text': record['QUESTION_TEXT'],\n",
    "                'answer_type': record['ANSWER_TYPE'],\n",
    "                'target_table': 'processed_formstack_data',\n",
    "                'target_field': 'message',\n",
    "                'constraints': answer_config\n",
    "            }\n",
    "            \n",
    "            batches[input_row_id]['questions'].append(question_data)\n",
    "            batches[input_row_id]['question_configs'][str(record['QUESTION_ID'])] = record.to_dict()\n",
    "        \n",
    "        logger.info(f\"Created {len(batches)} batches from {len(df)} pending records\")\n",
    "        return batches\n",
    "    \n",
    "    def _empty_result(self, start_time: float) -> Dict[str, Any]:\n",
    "        \"\"\"Return empty result structure.\"\"\"\n",
    "        return {\n",
    "            'status': 'completed',\n",
    "            'total_pending_records': 0,\n",
    "            'unique_batches': 0,\n",
    "            'batches_processed': 0,\n",
    "            'batches_succeeded': 0,\n",
    "            'batches_failed': 0,\n",
    "            'results_inserted': 0,\n",
    "            'processing_time_seconds': round(time.time() - start_time, 2)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
